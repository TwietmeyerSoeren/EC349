---
title: "Assignment 1 EC349"
author: "Sören Twietmeyer"
date: "`r Sys.Date()`"
output: html_document
---
<style>
body {
text-align: justify}
</style>
# EC349 Assignment 1: Analysis of Yelp Data
Note: The code can also be found on [GitHub](https://github.com/TwietmeyerSoeren/EC349)

## Outline of the goal
This project is aiming to predict user ratings on Yelp using data on the reviews submitted, the user, the business as well as data on additional comments made about a particular business. The goal is to achieve a precise prediction given the information available and does not require an evaluation of the reviews themselves, i.e., why users gave a specific review. This will allow us to trade off interpretability of our models for gains in performance. 

## Methodology
This data science project requires a methodology that is iterative, intuitive and suitable for an individual project. Given the absence of a business case, steps included in methods such as CRISP-DM relating to the understanding of the business are only marginally relevant. Additionally, CRISP-DM is documentation heavy, which can be helpful in longer and larger projects but is less required for an individual project. The John Rallin DS methodology, TDSP and the KDD methodology are very similar in structure and contain irrelevant parts regarding the business understanding and stakeholder involvement. The OSEMN methodology is the most appropriate since it only covers the core parts of the aforementioned methodologies and is well suited for individual projects which require less documentation. Nevertheless, I have adapted the structure to include a phase for a clear problem definition. This is crucial despite the absence of other parties with interest in the project.
```{r setup, warning=FALSE, include=FALSE, message=FALSE}
rm(list = ls())
library(conflicted)
library(jsonlite)
library(tidyverse)
library(dplyr)
library(tidyr)
library(tm)
library(hexbin)
library(doParallel)
library(glmnet)
library(fastDummies)
library(rpart)
library(tidymodels)
library(ggpubr)
library(xgboost)
library(SnowballC)
getwd()
conflicts_prefer(dplyr::summarise(), dplyr::filter, dplyr::mutate())
```
```{r Load data and functions, include = FALSE, message=FALSE}
# Load Different Data
business_data <- stream_in(file("../data/yelp_academic_dataset_business.json"))
business_data_unnested <- unnest(business_data, attributes)
checkin_data  <- stream_in(file("../data/yelp_academic_dataset_checkin.json"))
tip_data  <- stream_in(file("../data/yelp_academic_dataset_tip.json"))

# Load smaller data sets for comparative purposes and easier testing
load("../data/yelp_review_small.Rda")
load("../data/yelp_user_small.Rda")

# source functions
source("../code/assignment_functions.R")
```
```{r Data Exploration, include=FALSE, message=FALSE, warning=FALSE}
# business data
#str(business_data_unnested)
empty_rows_business_data <- count_empty_rows(business_data_unnested)
length(unique(business_data_unnested$business_id))
cols_keep_business_data <- empty_rows_business_data %>% filter(`EmptyRows` < 35000 & !Column %in% c('name', 'address', 'latitude', 'longitude'))

# review data
#str(review_data_small)
length(unique(review_data_small$review_id))

# check in data
#str(checkin_data)
check_ins_by_business <- checkin_data  %>% mutate(num_checkins_by_business = ifelse(date != "None", sapply(strsplit(date, ","), function(x) length(x)), 0)) %>% select(-"date")

# user data
#str(user_data_small)
length(unique(user_data_small$user_id))

# tip data
#str(tip_data)

# investigate relationships between different columns and stars_review
## tip data:
num_comments_by_business <- tip_data %>% group_by(business_id) %>% summarise(total_comments_business = n())

# user data:
user_data_selected <- user_data_small  %>% mutate(yelping_since_weeks = round(as.numeric(difftime("2023-12-31 00:00:00", yelping_since, units = "weeks")), digits = 0), num_friends = ifelse(friends != "None", sapply(strsplit(friends, ","), function(x) length(x)), 0), total_compliments = select(., starts_with("compliment_")) %>% rowSums(na.rm = TRUE)) %>% mutate(was_elite = ifelse(nchar(elite) > 1, 1, 0))%>%  select(-c(name, yelping_since, friends))

# check in data
check_ins_by_business <- checkin_data  %>% mutate(num_checkins_by_business = ifelse(date != "None", sapply(strsplit(date, ","), function(x) length(x)), 0)) %>% select(-"date")

# merge other data sets onto review data which is the main one:
review_data_combined <- review_data_small %>% left_join(business_data_unnested[, cols_keep_business_data$Column], by = 'business_id', suffix = c('_review', '_business')) %>% left_join(user_data_selected, by = 'user_id', suffix = c('_review', '_user')) %>% left_join(num_comments_by_business, by = 'business_id') %>% left_join(check_ins_by_business, by = "business_id") %>% mutate(BusinessAcceptsCreditCards = ifelse(BusinessAcceptsCreditCards == "True", 1, 0))

# see how columns are populated
#view(count_empty_rows(review_data_combined))

# business info:
business_review_comp <- ggplot(review_data_combined, mapping = aes(x = stars_business, y = stars_review)) + geom_hex() + geom_smooth(method = "glm", formula = y ~ x + I(x^2) + I(x^3)) + ylim(1,5) + labs(x = "Mean rating of a business", y = "User Rating") + theme_pubr()+ theme(legend.title = element_blank(), legend.position = 'right', legend.key.height = unit(1.5, 'cm'),text = element_text(colour = 'black', size = 12)) 

num_coms_business <- trend_plot("total_comments_business", 1)

city_plot <- ggplot(review_data_combined %>% group_by(city) %>% dplyr::summarise(mean_by_city = mean(stars_review)), mapping = aes(x = city, y= mean_by_city)) + geom_col() # clear differences in average ratings by city

state_plot <- ggplot(review_data_combined %>% group_by(state) %>% dplyr::summarise(mean_by_state = mean(stars_review)), mapping = aes(x = state, y= mean_by_state)) + geom_col() + labs(x = "State", y = "Mean Rating") + theme_pubr()+ theme(text = element_text(colour = 'black', size = 12)) # clear differences in average ratings by state

credit_card_plot <- trend_plot("BusinessAcceptsCreditCards", 0) # slight downward trend
mean(review_data_combined$BusinessAcceptsCreditCards, na.rm = TRUE)

is_open_plot <- trend_plot("is_open", 0) # slight upwards trend
mean(review_data_combined$is_open)

# review info:
usefulness_info_plot <- trend_plot("useful_review", 1)
funny_info_plot <- trend_plot("funny_review", 1)
cool_info_plot <- trend_plot("cool_review", 1)

review_count_plot <- trend_plot("review_count_review", 0) ## slight positive correlation, slightly non-linear

# user info: 
friends_count_plot <- trend_plot("num_friends", 1) ## slight positive correlation

fans_count_plot <- trend_plot("fans", 1) ## slight positive correlation

compliments_count_plot <- trend_plot("total_compliments", 1) ## slight positive correlation

yelp_since_plot <- trend_plot("yelping_since_weeks", 0) ## slight positive relationship
yelping_since_fancy_plot <- ggplot(review_data_combined, mapping = aes(x = yelping_since_weeks, y = stars_review)) +
  geom_hex() + geom_smooth(method = "glm", formula = y ~ x + I(x^2) + I(x^3)) + ylim(1,5) + labs(x = "Number of weeks since a user joined Yelp", y = "User Rating") + theme_pubr()+ theme(legend.title = element_blank(), legend.position = 'right', legend.key.height = unit(1, 'cm'),text = element_text(colour = 'black', size = 12)) 

# check-in data
num_checkins_plot <- ggplot(review_data_combined, mapping = aes(x = num_checkins_by_business, y = stars_review)) +
  geom_hex() + geom_smooth(method = "glm", formula = y ~ x + I(x^2) + I(x^3)) + ylim(1,5) + labs(x = "Number of check-ins made on a business", y = "User Rating", title = str_wrap("Relationship between the number of business check-ins and user ratings", 50)) + theme_pubr()+ theme(legend.title = element_blank(), legend.position = 'right', legend.key.height = unit(1, 'cm'),text = element_text(colour = 'black', size = 12)) 
```

## Data Exploration
The data exploration phase showed that the business data set includes useful information, however, some columns suffer from large shares of missing data, which negatively impacts its usefulness. Having a lot of missing values means that, if we want to include the variables, we need to drop a lot of reviews, leaving us with too few reviews, thus negatively impacting performance. Nevertheless, as shown in the graph below, the average business rating is highly positively correlated with the rating of a review and can thus serve as an important predictor. Similarly, businesses in different states tend to get different ratings, wherefore it is sensible to include state or city dummies in our models. However, given that there are more than 1400 cities and the linked risk of overfitting, the city variable will not be used. The characteristics useful, funny, cool show a non-linear relationship, which is why a squared and cubic term is added for each.  

```{r business review plot, echo=FALSE, warning=FALSE, fig.cap="Relationship between user ratings and average business ratings.", fig.align='center', fig.id=TRUE}
business_review_comp
```

```{r state plot, echo=FALSE, warning=FALSE, fig.cap="Average rating of businesses in each state.", fig.align='center', fig.id=TRUE}
state_plot
```

The tip data only includes additional comments made from a user to a business and can be omitted given the inclusion of the comments of the reviews from the review data set. The check-in data contains the times a user looked at a particular business. After aggregating the data, a slight negative non-linear relationship can be seen in the data, qualifying it as a feature.
Whilst the user data set contains valuable information that show significant correlation with our variable of interest such as the number of weeks since a user joined yelp (see graph below), there are only 20% of the users in the review data set present in the user data set. Therefore, we are unable to construct meaningful averages to fill the data or ignore the missing data points forcing us to omit the data entirely.

```{r yelping since plot, echo = FALSE, warning=FALSE, fig.cap="Relationship length of Yelp usership and ratings.", fig.align='center', fig.id=TRUE}
yelping_since_fancy_plot
```
```{r data manipulation, warning = FALSE, include = FALSE}
## Data Manipulation
# create one data frame that contains all the useful information 
## business data
cols_keep_business_data <- count_empty_rows(business_data_unnested) %>% filter(`EmptyRows` < 0.25*nrow(business_data_unnested) & !Column %in% c('name', 'address', 'latitude', 'longitude'))
business_data_selected <- dummy_cols(business_data_unnested[, cols_keep_business_data$Column], select_columns = c('state'), remove_selected_columns = FALSE, remove_first_dummy  = TRUE) %>% mutate(BusinessAcceptsCreditCards = ifelse(BusinessAcceptsCreditCards == "True", 1, 0))

check_ins_by_business <- checkin_data  %>% mutate(num_checkins_by_business = ifelse(date != "None", sapply(strsplit(date, ","), function(x) length(x)), 0)) %>% select(-"date")

num_comments_by_business <- tip_data %>% group_by(business_id) %>% summarise(total_comments_business = n())

# merge business_data, check_in_data, and tip_data onto review data which is the main one:
combined_df <- review_data_small %>% left_join(business_data_selected, by = 'business_id', suffix = c('_review', '_business')) %>% left_join(num_comments_by_business, by = 'business_id') %>% left_join(check_ins_by_business, by = "business_id")

# find all columns that have enough populated cells, drop the rest
combined_df_empty_rows <- count_empty_rows(combined_df) %>% filter(`EmptyRows` < 0.2*nrow(combined_df))

# drop all rows that have any remaining missing values after the columns have been selected since the models can't handle missing values
combined_df_filtered <- combined_df[, combined_df_empty_rows$Column] %>% na.omit()

# This reduces the file sizes due to the lack of sufficient computing power, to be removed if more is available
set.seed(1)
file_reduction_indeces <- sample(1:nrow(combined_df_filtered), nrow(combined_df_filtered) - 900000)
combined_df_sliced <-combined_df_filtered[file_reduction_indeces,]

## add the text from the comments in a cleaned way such that it can be included in our models
df_part_one <- generate_final_df(1, 100000, combined_df_sliced)
df_part_two <- generate_final_df(100001, 200000, combined_df_sliced)
df_part_three <- generate_final_df(200001, nrow(combined_df_sliced), combined_df_sliced)

# save the data frame so that we don't have to generate it over and over again
df_combined <- bind_rows(df_part_one, df_part_two, df_part_three)

# manipulate data so that columns get filled with 0s
cols_to_change <- colnames(df_combined)[46:length(colnames(df_combined))]
df_combined_updated <- df_combined %>% mutate_at(vars(cols_to_change), ~replace_na(., 0))

```
## Modelling
```{r test and training set generation, include = FALSE}
## Modelling
final_dataset <- df_combined_updated %>%  select(-c("date", "text", starts_with("compliment_"), "review_id", "business_id", "user_id", "city", "postal_code", "categories", "state")) %>% mutate(useful_sq = I(useful^2), useful_cube = I(useful^3), funny_sq = I(funny^2), funny_cube = I(funny^3), cool_review_sq = I(cool_review^2), cool_review_cube = I(cool_review^3), num_checkins_by_business_sq = I(num_checkins_by_business^2), num_checkins_by_business_cube = I(num_checkins_by_business^3))

#Split data into test and training
set.seed(1)
train <- sample(1:nrow(final_dataset), nrow(final_dataset) - 10000) #keep 10,000 for the test data
data_train <-final_dataset[train,]

#Test data
data_test <- final_dataset[-train,]

# for shrinkage estimators we need to transform data into matrices, therefore we need the following transformation: 
data_train_x <-data_train[,!colnames(data_train) %in% c("stars_review")]
data_train_y <-data_train[,c("stars_review")]

data_test_x <-data_test[, !colnames(data_test) %in% c("stars_review")]
data_test_y <-data_test[,c("stars_review")]
```
```{r modelling and performance testing, warning=FALSE, include=FALSE}
### Linear Models
lm_stars <- lm(stars_review ~., data = data_train)

# Evaluation of model
model_performance_lm_training <- summary(lm_stars)

#Prediction to test data
lm_stars_predict<-predict(lm_stars, newdata = data_test[, !colnames(data_test) %in% c("stars_review")])

#Evaluation of lm on test:
metrics <- metric_set(rmse, rsq)
model_performance_lm <- data_test %>%
 mutate(predictions = lm_stars_predict) %>%
 metrics(truth = stars_review, estimate = predictions)

## LASSO and Ridge
ridge_results <- shrinkage_estimator_computation(0, data_train_x, data_train_y, data_test_x, data_test_y)
cv_out_ridge <- ridge_results$cv_out
plot_cv_out_ridge <- ridge_results$cv_out_plot
lambda_ridge_cv<- ridge_results$lambda_cv
ridge_model<- ridge_results$model
ridge_mse <- ridge_results$mse
ridge_mse_training <- ridge_results$mse_training
ridge_rsq <- ridge_results$rsq
ridge_rsq_training <- ridge_results$rsq_training
coefficients_ridge <- ridge_results$coefficients_model
highest_lowest_coefficients_ridge <- ridge_results$coef_model_df_high_low

coef_plot_ridge <- ggplot(data = highest_lowest_coefficients_ridge, mapping = aes(reorder(term, coefficient, sum), coefficient)) + geom_col() + labs(x = "Word used", y = "Coefficient", title = "Coefficients of words used in review")

#LASSO with Cross-Validation
lasso_results <- shrinkage_estimator_computation(1, data_train_x, data_train_y, data_test_x, data_test_y)
cv_out_lasso <- lasso_results$cv_out
plot_cv_out_lasso <- lasso_results$cv_out_plot
lambda_lasso_cv <- lasso_results$lambda_cv
lasso_model<- lasso_results$model
lasso_mse <- lasso_results$mse
lasso_mse_training <- lasso_results$mse_training
lasso_rsq <- lasso_results$rsq
lasso_rsq_training <- lasso_results$rsq_training
coefficients_lasso <- lasso_results$coefficients_model
highest_lowest_coefficients_lasso <- lasso_results$coef_model_df_high_low

coef_plot_lasso <- ggplot(data = highest_lowest_coefficients_lasso, mapping = aes(reorder(term, coefficient, sum), coefficient)) + geom_col() + labs(x = "Word used", y = "Coefficient", title = "Coefficients of words used in review")


## Comparison of LASSO and ridge with frequency matrix of words replaced by binary (used/ not used) matrix
adapted_df <- final_dataset %>% mutate_at(vars(colnames(final_dataset[, 38:ncol(final_dataset)-8])), list(~ifelse(. > 0, 1, 0)))
# generate new training and test datasets
data_train_binary <-adapted_df[train,]

#Test data
data_test_binary <- adapted_df[-train,]

# for shrinkage estimators we need to transform data into matrices, therefore we need the following transformation: 
data_train_binary_x <-data_train_binary[,!colnames(data_train_binary) %in% c("stars_review")]
data_train_binary_y <-data_train_binary[,c("stars_review")]

data_test_binary_x <-data_test_binary[, !colnames(data_test_binary) %in% c("stars_review")]
data_test_binary_y <-data_test_binary[,c("stars_review")]

# application on LASSO
#LASSO with Cross-Validation
lasso_results_binary <- shrinkage_estimator_computation(1, data_train_binary_x, data_train_y, data_test_binary_x, data_test_binary_y)
cv_out_lasso_binary <- lasso_results_binary$cv_out
plot_cv_out_lasso_binary <- lasso_results_binary$cv_out_plot
lambda_lasso_cv_binary <- lasso_results_binary$lambda_cv
lasso_model_binary <- lasso_results_binary$model
lasso_mse_binary <- lasso_results_binary$mse
lasso_mse_training_binary <- lasso_results_binary$mse_training
lasso_rsq_binary <- lasso_results_binary$rsq
lasso_rsq_training_binary <- lasso_results_binary$rsq_training

### Non-Linear Models
# classic decision tree model without pruning
standard_tree_spec <- rpart(stars_review ~ ., data = data_train)

predictions_tree <- predict(standard_tree_spec, newdata = data_test)

# Calculate RMSE and R-squared
model_performance <- data_test %>%
 mutate(predictions = predictions_tree) %>%
 metrics(truth = stars_review, estimate = predictions)

# Fit an xgboost model
xgb_model <- xgboost(data = as.matrix(data_train[, !colnames(data_train) %in% c("stars_review")]),
                      label = data_train$stars_review,
                      nrounds = 100,
                      verbose = 0,
                      objective = "reg:squarederror", 
                      eta = 0.25, 
                      max_depth = 8)

# Predictions on the test data
predictions_xgboost <- predict(xgb_model, as.matrix(data_test[, !colnames(data_train) %in% c("stars_review")]))

model_performance_xgboost <- data_test %>%
 mutate(predictions = predictions_xgboost) %>%
 metrics(truth = stars_review, estimate = predictions)

```

### Model choice
Whilst the reviews are given in integers, it is an ordered variable, which nevertheless has an underlying continuous meaning. Therefore, we can use regression methods to predict the outcome instead of classification methods.
Our data set contains a small number of parameters relative to a large number of observations. However, only few variables such as the average number of stars given to a business are likely to be highly correlated with our variable of interest. In these cases, models performing variable selection/ shrinking will do better since they are able to largely reduce the variance which emerges due to the presence of many parameters. Ridge and LASSO models both benefit from the ability to shrink coefficients to pick the most predictive ones hence reducing the negative impact of variables of low explanatory power. Additionally, the text is unlikely to be related in a highly non-linear way, wherefore the linear models perform well. This is amplified by the inclusion of non-linear terms that account for some of the non-linear relationships and by the fact that the strongest predictors from the review and business characteristics are linearly related to the outcome. 

### Performance evaluation
#### Linear models

| Performance/Model   | LRM           |  LASSO | Ridge  | LASSO binary |
|:--------------------|:-------------:|:------:|:------:|:------:|
| MSE on test data    |0.96           | 0.96   |0.97    | 0.91   |
| MSE on training data| 0.97          | 0.95   |0.96    | 0.88   |
| $R^2$ on test data  | 0.54          | 0.53   |0.54    | 0.56   |
| $R^2$ on training data|0.54         | 0.54   |0.54    | 0.57   |

Ridge and LASSO can shrink the estimators to reduce the variance of the estimators. However, given the large number of observations and relatively small number of parameters, the variance of the OLS estimator is already relatively low. In combination with the cross-validated lambda being very close to zero for both Ridge and LASSO, this explains why they don't outperform OLS. The small decrease in variance that can be obtained through shrinking is made redundant by the increase in bias generated by the shrinkage methods.
A better performance can be achieved by changing the data format slightly to only indicate the usage of a word without specifying the frequency. This is due to the fact that often used words are prevented from getting too much weight.

Including the text of reviews leads to a significant increase in performance given the relatively large informativeness of some words as shown in the graph of the coefficients for shrinkage estimators.
```{r coefficient plot of SE ,echo=FALSE, warning=FALSE, fig.cap="The seven highest and seven lowest LASSO coefficients on the text used in reviews.", fig.align='center', fig.id=TRUE}
coef_plot_lasso
```
#### Non-linear models
Simple regression trees are unlikely to perform well given that there are over 500 parameters, most of which have low levels of informativeness. Under these circumstances, a large tree would be required, leading to a high variance and poor performance. Using Random Forests to decrease the variance will lead to poor performance due to the large number of parameters and few good predictors, as evidenced by the fact that only around 12% of features have a an absolute value of the coefficient larger than 0.1. This would lead to strongly biased trees in cases when only poor predictors are chosen at each node and given the high probability of this happening, the bias is likely to remain. 
Nevertheless, boosting is able to achieve a performance comparable to the shrinkage estimators. This is due to the combination of bagging, learning and the ability to capture non-linear relationships between the variables. Bagging reduces the variance of the estimates by using averages and by letting the estimator explain the previously unexplained data points, enabling us to achieve a more precise estimate. By reducing the learning rate, we can achieve a lower MSE due to the prevention of overfitting. 

| Performance/Model   | Unpruned Tree|  Boosted Tree |
|:--------------------|:-------------:|:------:|
| MSE on test data    |1.21|0.86|
| $R^2$ on test data  | 0.30             | 0.64   |

The merely marginal difference between the linear and non-linear methods can be explained by the likely linear relationship between the explanatory and outcome variables. It is unlikely that the variables such as the words are related in a highly non-linear way in which case boosted trees would perform better. Additionally, the strong predictors such as business characteristics are related in a linear way further highlighting why more flexible models don't significantly outperform linear models. 

## Project Challenges
A major challenge during the project was to determine what data structure is required, how the data can be transformed to be made useful and how the ultimate goal of prediction can best be reached. Due to the lack of prior experience in text analysis, understanding how to prepare text and which models perform well with text-based data was challenging. However, using an iterative approach of exploring the data, assessing its usefulness whilst referring to the initial goal has proven itself particularly instructive. Applying the same strategy to the modelling phase was equally effective.

## Statement of Academic Integrity
We're part of an academic community at Warwick.

Whether studying, teaching, or researching, we’re all taking part in an expert conversation which must meet standards of academic integrity. When we all meet these standards, we can take pride in our own academic achievements, as individuals and as an academic community.

Academic integrity means committing to honesty in academic work, giving credit where we've used others' ideas and being proud of our own achievements.

In submitting my work I confirm that:

1. I have read the guidance on academic integrity provided in the Student Handbook and understand the University regulations in relation to Academic Integrity. I am aware of the potential consequences of Academic Misconduct.

2. I declare that the work is all my own, except where I have stated otherwise.

3. No substantial part(s) of the work submitted here has also been submitted by me in other credit bearing assessments courses of study (other than in certain cases of a resubmission of a piece of work), and I acknowledge that if this has been done this may lead to an appropriate sanction.

4. Where a generative Artificial Intelligence such as ChatGPT has been used I confirm I have abided by both the University guidance and specific requirements as set out in the Student Handbook and the Assessment brief. I have clearly acknowledged the use of any generative Artificial Intelligence in my submission, my reasoning for using it and which generative AI (or AIs) I have used. Except where indicated the work is otherwise entirely my own.

5. I understand that should this piece of work raise concerns requiring investigation in relation to any of points above, it is possible that other work I have submitted for assessment will be checked, even if marks (provisional or confirmed) have been published.

6. Where a proof-reader, paid or unpaid was used, I confirm that the proofreader was made aware of and has complied with the University’s proofreading policy.

7. I consent that my work may be submitted to Turnitin or other analytical technology. I understand the use of this service (or similar), along with other methods of maintaining the integrity of the academic process, will help the University uphold academic standards and assessment fairness.

Privacy statement

The data on this form relates to your submission of coursework. The date and time of your submission, your identity, and the work you have submitted will be stored. We will only use this data to administer and record your coursework submission.

Related articles:

[Reg. 11 Academic Integrity (from 4 Oct 2021)](https://warwick.ac.uk/services/gov/calendar/section2/regulations/academic_integrity/)

[Guidance on Regulation 11](https://warwick.ac.uk/services/aro/dar/quality/az/acintegrity/framework/guidancereg11/)

[Proofreading Policy](https://warwick.ac.uk/services/aro/dar/quality/categories/examinations/policies/v_proofreading/)

[Education Policy and Quality Team](https://warwick.ac.uk/services/aro/dar/quality/az/acintegrity/framework/guidancereg11/)

[Academic Integrity](https://warwick.ac.uk/students/learning-experience/academic_integrity)